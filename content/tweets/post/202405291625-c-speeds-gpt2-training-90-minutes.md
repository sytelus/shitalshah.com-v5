---
title: 'When ''C'' Speeds: GPT-2 Training Cut from 14 Hours to 90 Minutes'
draft: false
date: 2024-05-29T16:25:30+00:00
slug: '202405291625-c-speeds-gpt2-training-90-minutes'
is_tweet: true
tweet_info:
  id: '1795748291692642400'
  type: 'post'
  is_thread: False
---



Training GPT2 124M params in 90 minutes is stunning! For reference, PyTorch+FlashAttention used to take 14hr on same 8xA100 and 10B tokens! !

I actually didn't thought rewriting everything in C would make a lot of difference because Python/PyTorch aren't exactly the bottlenecksâ€¦ <https://x.com/karpathy/status/1795484547267834137>

[Discussion](https://x.com/sytelus/status/1795748291692642400)
