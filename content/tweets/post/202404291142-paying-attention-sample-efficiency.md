---
title: 'Paying Attention to Sample Efficiency'
draft: false
date: 2024-04-29T11:42:20+00:00
slug: '202404291142-paying-attention-sample-efficiency'
is_tweet: true
tweet_info:
  id: '1784805391614009745'
  type: 'post'
  is_thread: False
---



While many architectures can reach performance of Transformer, the right question to ask is at what sample efficiency. 

Attention+backprop is the most sample efficient setting we have discovered so far but my guess is that we are orders of magnitudes away from whatâ€™s possible. <https://x.com/jxmnop/status/1784696357892063565>

[Discussion](https://x.com/sytelus/status/1784805391614009745)
