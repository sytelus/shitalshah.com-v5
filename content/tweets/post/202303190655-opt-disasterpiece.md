---
title: 'The OPT Disasterpiece: LLM Training Gone Wrong'
draft: false
date: 2023-03-19T06:55:29+00:00
slug: '202303190655-opt-disasterpiece'
is_tweet: true
tweet_info:
  id: '1637241343620833280'
  type: 'post'
  is_thread: False
---



I havenâ€™t read a paper with more chaos and disasters while training LLM than OPT. It could be because they bravely chose to add those details but my guess is rather some of the bad choices they made  like init, linear LR schedule, ReLUs, buggy loss scaling etc.

[Discussion](https://x.com/sytelus/status/1637241343620833280)
