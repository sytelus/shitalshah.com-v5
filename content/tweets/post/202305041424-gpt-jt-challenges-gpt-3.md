---
title: '6B Params and a Dream: GPT-JT Challenges GPT-3'
draft: false
date: 2023-05-04T14:24:30+00:00
slug: '202305041424-gpt-jt-challenges-gpt-3'
is_tweet: true
tweet_info:
  id: '1654024184354463745'
  type: 'post'
  is_thread: False
---



GPT-JT is fine tuned version of GPT-J with ~1B tokens of instruction/prompt datasets and ~2B Pile tokens using UL2 objective. This results in 6B param model that *might be* competitive with GPT-3 (no study on contamination/overfit as usual).

<https://huggingface.co/togethercomputer/GPT-JT-6B-v1>

[Discussion](https://x.com/sytelus/status/1654024184354463745)
