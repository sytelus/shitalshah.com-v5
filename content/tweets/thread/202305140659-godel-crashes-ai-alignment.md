---
title: 'When Gödel Crashes the AI Alignment Party'
draft: false
date: 2023-05-14T06:59:36+00:00
slug: '202305140659-godel-crashes-ai-alignment'
is_tweet: true
tweet_info:
  id: '1657536101794598912'
  type: 'thread'
  is_thread: True
---



Below little prompt has such a deep consequences. I have been thinking about this for few hours now. 

It seems that strongly aligned AI can be similarly vulnerable as weakly aligned!

There is probably a theorem for AI alignment similar to Gödel’s incompleteness theorem. <https://x.com/sytelus/status/1657483822181785600>

I tried similar prompts out on GPT-4. You can take it to next level by claiming to have nukes :). Unfortunately (or fortunately), GPT-4 inference pipeline has filter that seems detects these stuff, cuts you off and doesn’t allow interactions with the model.

[Discussion](https://x.com/sytelus/status/1657536101794598912)
