---
title: 'Byte-Level Tokenizers: Four Times the Tokens, Four Times the Trouble'
draft: true
date: 2023-12-01T13:45:03+00:00
slug: '202312011345-byte-level-tokenizers-4x-trouble'
is_tweet: true
tweet_info:
  id: '1730462998257639563'
  type: 'reply'
  is_thread: False
---



{{< tweet user="andrew_n_carr" id="1730427293611511816" >}}

I believed this too 6 months ago but after some small scale experiments, I am starting to change my mind. Byte level tokenizer is just not workable. First, will need to increase context length to about 4X to see same info. Second, you will need to train on 4X more tokens. So,â€¦ [continue reading](https://x.com/sytelus/status/1730462998257639563)
