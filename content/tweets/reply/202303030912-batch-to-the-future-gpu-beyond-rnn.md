---
title: 'Batch to the Future: Unlocking GPU Power Beyond RNN Iteration'
draft: true
date: 2023-03-03T09:12:15+00:00
slug: '202303030912-batch-to-the-future-gpu-beyond-rnn'
is_tweet: true
tweet_info:
  id: '1631462457024864256'
  type: 'reply'
  is_thread: False
---



{{< tweet user="rasbt" id="1631286616596652032" >}}

Authors argued in original paper that main form of parallelism comes ability to batch data during training which fully utilizes GPU (as opposed to RNNs where you must iterate).

[Discussion](https://x.com/sytelus/status/1631462457024864256)
