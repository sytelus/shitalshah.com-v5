---
title: 'LLaMA Drama: The Code Token Conundrum'
draft: true
date: 2023-04-18T11:05:28+00:00
slug: '202304181105-llama-drama-code-token-conundrum'
is_tweet: true
tweet_info:
  id: '1648175888574599168'
  type: 'reply'
  is_thread: False
---



{{< tweet user="togethercompute" id="1648087122379960321" >}}

[@sampullara](https://x.com/sampullara) The problem is that LLaMA made huge mistake for not having enough code tokens. So, this dataset will perpetuate that mistake. I would highly recommend generating at least 100B tokens worth of additional code data and keeping it separate so people can reproduce LLaMA or go beyond.

[Discussion](https://x.com/sytelus/status/1648175888574599168)
