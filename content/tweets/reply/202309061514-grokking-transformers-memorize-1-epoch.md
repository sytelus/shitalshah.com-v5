---
title: 'Grokking: When Transformers Memorize in 1 Epoch'
draft: true
date: 2023-09-06T15:14:27+00:00
slug: '202309061514-grokking-transformers-memorize-1-epoch'
is_tweet: true
tweet_info:
  id: '1699335236944089224'
  type: 'reply'
  is_thread: False
---



{{< tweet user="jeremyphoward" id="1699216721473880425" >}}

[@johnowhitaker](https://x.com/johnowhitaker) I can reproduce this behavior in grokking. The synthetic dataset of 24k samples is completely memorized by 450K params transformer in just 1 epoch! The training takes about 20 seconds.

Yes, you read these numbers right :). 

Let me know if you are interested in this experiment.

[Discussion](https://x.com/sytelus/status/1699335236944089224)
