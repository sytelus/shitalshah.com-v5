---
title: 'PyTorch: Ready for Transformer Overload?'
draft: true
date: 2023-01-11T16:59:54+00:00
slug: '202301111659-pytorch-transformer-overload'
is_tweet: true
tweet_info:
  id: '1613098365851701248'
  type: 'reply'
  is_thread: False
---



[@DrGroftehauge](https://x.com/DrGroftehauge) [@jmes_harrison](https://x.com/jmes_harrison) [@Luke_Metz](https://x.com/Luke_Metz) [@bucketofkets](https://x.com/bucketofkets) [@poolio](https://x.com/poolio) [@jaschasd](https://x.com/jaschasd) [@ada_rob](https://x.com/ada_rob) [@IMordatch](https://x.com/IMordatch) [@amilmerchant](https://x.com/amilmerchant) [@jekbradbury](https://x.com/jekbradbury) [@naman33k](https://x.com/naman33k) Does this work with PyTorch? Also, can it handle transformer architectures generated by choosing arbitrary parameters such as N layers, H heads etc?

[Discussion](https://x.com/sytelus/status/1613098365851701248)
