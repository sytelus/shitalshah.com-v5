---
title: 'Little LSTM Outsmarts Big Transformer in Flip-Flop Task'
draft: false
date: 2023-06-09T16:45:38+00:00
slug: '202306091645-lstm-vs-transformer-flipflop'
is_tweet: true
tweet_info:
  id: '1667105663435300871'
  type: 'post'
  is_thread: False
---



Very interesting simple scenario where a small LSTM works perfectly but 20X larger transformer architecture fails!

Exposing Attention Glitches with Flip-Flop Language Modeling
<https://arxiv.org/abs/2306.00946>

[Discussion](https://x.com/sytelus/status/1667105663435300871)
