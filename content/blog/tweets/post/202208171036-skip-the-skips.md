---
title: 'Skip the Skips: 500-Layer Neural Nets Made Simple'
draft: false
date: 2022-08-17T10:36:08+00:00
slug: '202208171036-skip-the-skips'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1559745868609097729'
  type: 'post'
  is_thread: False
---



Huh??!! “we can therefore train a "vanilla" fully connected network and convolutional neural network—no skip connections, batch normalization, dropout, or any other architectural tweak—with 500 layers by simply adding the batch-entropy regularization term to the loss function.” <https://x.com/_arohan_/status/1559709611313094656>

[Discussion](https://x.com/sytelus/status/1559745868609097729)
