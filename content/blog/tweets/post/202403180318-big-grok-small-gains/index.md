---
title: 'Big Grok, Small Gains: Grok-1 Code Release Underwhelms'
draft: false
date: 2024-03-18T03:18:22+00:00
slug: '202403180318-big-grok-small-gains'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1769458277308571676'
  type: 'post'
  is_thread: False
---



Grok-1 code is released. It's 8 experts, 2 selected at a time. Trend of large vocab (131k) continues. Attention output multiplier is interesting. Overall, it's much large model than I'd thought so bit surprised about lag in perf than other models. <https://x.com/ibab/status/1769447989192675748> 

![https://pbs.twimg.com/media/GI5g7MRaQAAKEyv.jpg](o6ZAUGLXiP.jpg)

[Discussion](https://x.com/sytelus/status/1769458277308571676)
