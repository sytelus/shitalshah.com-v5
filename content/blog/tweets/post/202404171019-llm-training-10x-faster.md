---
title: 'Pretraining has Gotten 10x Faster in Past Year!'
draft: false
date: 2024-04-17T10:19:14+00:00
slug: '202404171019-llm-training-10x-faster'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1780435825941160212'
  type: 'post'
  is_thread: False
---



While digging up some historical numbers, it hit me that LLM training is now ~10X faster than same time last year from tons of improvements like H100 availability, Flash Attention2, new kernels, torch.compile, CUDA graphs, FP8 etc!

That's just past 12 months!!

[Discussion](https://x.com/sytelus/status/1780435825941160212)
