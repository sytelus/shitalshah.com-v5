---
title: 'Byte Feeding Frenzy: Spiky Training with Token Overdose'
draft: true
date: 2024-02-16T15:54:39+00:00
slug: '202402161554-byte-feeding-frenzy'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1758399478779019712'
  type: 'post'
  is_thread: False
---



Quite interestingly if you  feed model bytes directly, you will be using ~5X more tokens. While this does allow you to train a model
on any modality in theory, I’d found training was rather flaky with too many spikes. Need to dust off that code 🧑‍💻. <https://x.com/JeremyNguyenPhD/status/1758332850922045932>

[Discussion](https://x.com/sytelus/status/1758399478779019712)
