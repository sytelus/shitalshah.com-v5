---
title: 'Frozen Weights, Hot Results: Batch Norm Gets 83% on CIFAR-10'
draft: false
date: 2020-04-20T00:36:34+00:00
slug: '202004200036-frozen-weights-hot-results'
is_tweet: true
tweet_info:
  id: '1251927683765645318'
  type: 'post'
  is_thread: False
---



Turns out that if you froze all layers of neural networks to their random initialized weights except for batch norms, you can still get 83% accuracy on cifar10!

<https://arxiv.org/abs/2003.00152>

[Discussion](https://x.com/sytelus/status/1251927683765645318)
