---
title: 'The Transformer Paradox: Overtrained but Not Overfit'
draft: false
date: 2024-10-04T04:57:26+00:00
slug: '202410040457-transformer-paradox-overtrained-not-overfit'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1841960777588379656'
  type: 'post'
  is_thread: False
---



There is something about transformer architecture whereby even a ton of overtraining doesn't cause significant overfitting. 

No other architecture features this property. Most ML folks had assumed that was simply not possible.

Source: <https://arxiv.org/abs/2410.01201> 

![https://pbs.twimg.com/media/GY_0vkZbAAEMPbO.png](zpxYY1eJge.png)

[Discussion](https://x.com/sytelus/status/1841960777588379656)
