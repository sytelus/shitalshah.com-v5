---
title: 'Big Models Love Tech Data: Scaling Laws Strike Again'
draft: false
date: 2021-01-02T13:39:32+00:00
slug: '202101021339-big_models_love_tech_data'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1345243308985995266'
  type: 'post'
  is_thread: False
---



Dataset for training language models using 22 sources. Great to see technical sources like pubmed, arxiv, stackexchange, GitHub. Models trained on common crawl donâ€™t perform as good on technical language but surprisingly scaling law still applies, i.e., larger models do better. <https://x.com/nabla_theta/status/1345130408170541056>

[Discussion](https://x.com/sytelus/status/1345243308985995266)
