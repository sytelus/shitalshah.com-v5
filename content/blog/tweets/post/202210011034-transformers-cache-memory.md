---
title: 'Cache Me If You Can: Giving Transformers a Memory'
draft: false
date: 2022-10-01T10:34:01+00:00
slug: '202210011034-transformers-cache-memory'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1576052788814979072'
  type: 'post'
  is_thread: False
---



This idea looks very cool: Extend transformer with large cache to store data at inference time (no weight change). One can then feed transformer with series of new facts which will be cached and used in subsequent inference. Memory is key missing piece in current architectures. <https://x.com/ChrSzegedy/status/1503906876416798722>

[Discussion](https://x.com/sytelus/status/1576052788814979072)
