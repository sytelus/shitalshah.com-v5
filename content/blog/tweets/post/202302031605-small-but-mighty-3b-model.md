---
title: 'Small But Mighty: 3B Model Outsmarts 58X Larger LLM'
draft: false
date: 2023-02-03T16:05:23+00:00
slug: '202302031605-small-but-mighty-3b-model'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1621419563559833601'
  type: 'post'
  is_thread: False
---



Instruction tuning is proving to the most important weapon to reduce size of LLMs. Last year we show T0 model beating 16X larger GPT-3 on zero shot. Below paper now pushes the limits with mere 3B model beating 58X larger OPT-IML!! 

Single GPU LLMs getting closer! <https://x.com/ShayneRedford/status/1620805305801261058>

[Discussion](https://x.com/sytelus/status/1621419563559833601)
