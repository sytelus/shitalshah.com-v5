---
title: 'Self-Attention Slimdown: From O(N²) to O(N·M), But Is It Worth It?'
draft: false
date: 2021-02-18T12:48:27+00:00
slug: '202102181248-self-attention-slimdown'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1362262679457464322'
  type: 'post'
  is_thread: False
---



Interesting take on reducing self-attention complexity from O(N^2) to O(N*M) although results don't look eye popping compared to SOTA like DeiTs. <https://x.com/_akhaliq/status/1362221635571433481>

[Discussion](https://x.com/sytelus/status/1362262679457464322)
