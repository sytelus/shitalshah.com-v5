---
title: 'Learning Rate Warmup: The Hot New Stabilizer'
draft: false
date: 2021-10-14T02:20:45+00:00
slug: '202110140220-learning-rate-warmup'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1448368127452999680'
  type: 'post'
  is_thread: False
---



“learning rate warmup can improve training stability just as much as batch normalization, layer normalization, MetaInit, GradInit, and Fixup initialization” <https://x.com/Arxiv_Daily/status/1448198932949921801>

[Discussion](https://x.com/sytelus/status/1448368127452999680)
