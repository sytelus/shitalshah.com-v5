---
title: 'PaLM2-L''s Token Diet: Outsmarting Chinchilla and LLaMA'
draft: false
date: 2023-05-18T05:57:09+00:00
slug: '202305180557-palm2-token-diet'
is_tweet: true
tweet_info:
  id: '1658969933970288641'
  type: 'post'
  is_thread: False
---



Below leaked PaLM numbers are bit unusual. It points to new scaling law paper eluded to.

 - Chinchilla optimal tokens for 340B is 8.7T. 

- LLaMA optimal tokens is at least 26T.

This means PaLM2-L needed 2.5X less tokens by Chinchilla standards and even less by LLaMA standards. <https://x.com/ml_hardware/status/1658936724943142913>

[Discussion](https://x.com/sytelus/status/1658969933970288641)
