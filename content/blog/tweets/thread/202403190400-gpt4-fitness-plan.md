---
title: 'GPT-4''s New Fitness Plan: From 25K to 2K GPUs'
draft: false
date: 2024-03-19T04:00:02+00:00
slug: '202403190400-gpt4-fitness-plan'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1769831148694225350'
  type: 'thread'
  is_thread: True
---



If you were to train GPT-4, 1.8T params model, 

On A100, it will take 25k A100s and take 3-5 months.

On H100, it will take 8k GPUs and take ~3 months.

On B100, it will take 2k GPUs and take ~ 3 months.

- Jenson at GTC.

One of the big consequence here is that you donâ€™t have span multiple colos to train GPT-4 class models. This significantly reduces complexity and puts this class of models right in to hands of many startups.

[Discussion](https://x.com/sytelus/status/1769831148694225350)
