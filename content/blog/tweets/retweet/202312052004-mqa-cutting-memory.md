---
title: 'MQA: Cutting Memory, But at What Cost?'
draft: true
date: 2023-12-05T20:04:33+00:00
slug: '202312052004-mqa-cutting-memory'
is_tweet: true
tweet_info:
  id: '1732008053208387597'
  type: 'retweet'
  is_thread: False
---



RT [@d_haziza](https://x.com/d_haziza): Many LLMs today use Multi-Query Attention (MQA). The goal is to reduce memory usage from KV-cache at inference, but it also mâ€¦ [continue reading](https://x.com/sytelus/status/1732008053208387597)
