---
title: 'LLaMA Goes Long: Scaling to Over 8k Contexts in Transformers'
draft: true
date: 2023-07-14T15:58:24+00:00
slug: '202307141558-llama-goes-long-8k-contexts'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1679777354749706240'
  type: 'retweet'
  is_thread: False
---



RT [@joao_gante](https://x.com/joao_gante): Scaling LLaMA and GPTNeoX to &gt;8k input context -- you can now get it from ðŸ¤— transformers!

The best part? You can achieve iâ€¦ [continue reading](https://x.com/sytelus/status/1679777354749706240)
