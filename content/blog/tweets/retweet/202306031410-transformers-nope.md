---
title: 'Transformers Say NoPE to Positional Encoding'
draft: true
date: 2023-06-03T14:10:26+00:00
slug: '202306031410-transformers-nope'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1664892282128982017'
  type: 'retweet'
  is_thread: False
---



RT [@a_kazemnejad](https://x.com/a_kazemnejad): 🚨Stop using positional encoding (PE) in Transformer decoders (e.g. GPTs). Our work shows 𝗡𝗼𝗣𝗘 (no positional encoding) ou… [continue reading](https://x.com/sytelus/status/1664892282128982017)
