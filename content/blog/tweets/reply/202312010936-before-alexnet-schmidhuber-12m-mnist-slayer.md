---
title: 'Before AlexNet: Schmidhuber''s 12M-Parameter MNIST Slayer'
draft: true
date: 2023-12-01T09:36:30+00:00
slug: '202312010936-before-alexnet-schmidhuber-12m-mnist-slayer'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1730400445104095727'
  type: 'reply'
  is_thread: False
---



{{< tweet user="SchmidhuberAI" id="1730255400740483357" >}}

One of my favorite Schmidhuber paper is actually <https://arxiv.org/abs/1003.0358>. In 2010, 2 years before AlexNet, they trained 12M param model to beat popular MNIST benchmark at the time. They realized such “large” 9 layer MLP would take weeks to train on CPU because MNIST had whooping… [continue reading](https://x.com/sytelus/status/1730400445104095727)
