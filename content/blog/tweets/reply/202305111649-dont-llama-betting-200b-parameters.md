---
title: 'Don''t LLaMA: Betting on a 200B-Parameter Model'
draft: true
date: 2023-05-11T16:49:27+00:00
slug: '202305111649-dont-llama-betting-200b-parameters'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1656597377464287232'
  type: 'reply'
  is_thread: False
---



{{< tweet user="ml_hardware" id="1656510710988541952" >}}

My bet is on 200B params. PaLM was trained on 750B. They added more languages, Minerva datasets, code datasets. I would be very surprised if they trained on less than LLaMA tokens (1.4T). Per their scaling law plots, this would be around 200B params.

[Discussion](https://x.com/sytelus/status/1656597377464287232)
