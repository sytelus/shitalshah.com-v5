---
title: 'FSDP and DeepSpeed Zero Break Up with 7B+ Models'
draft: true
date: 2024-02-11T15:33:33+00:00
slug: '202402111533-fsdp-deepspeed-7b-models'
is_tweet: true
tweet_info:
  id: '1756582227725767073'
  type: 'reply'
  is_thread: False
---



{{< tweet user="StasBekman" id="1754372753569001492" >}}

[@jeremyphoward](https://x.com/jeremyphoward) [@PhoBoAI](https://x.com/PhoBoAI) [@allen_ai](https://x.com/allen_ai) [@Muennighoff](https://x.com/Muennighoff) FSDP or DeepSpeed Zero won't cut it for 7B+ though. Also, for GPUs &gt; 256, global batch size would blow up requiring other parallelisms.

[Discussion](https://x.com/sytelus/status/1756582227725767073)
