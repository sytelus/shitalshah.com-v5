---
title: 'SHA as a Loss Function? Not So Hot'
draft: true
date: 2020-03-26T18:02:34+00:00
slug: '202003261802-sha-not-so-hot'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1243131220994609154'
  type: 'reply'
  is_thread: False
---



{{< tweet user="theshawwn" id="1242978907814367233" >}}

[@eigenhector](https://x.com/eigenhector) This might work but might slow down the training. For one hot, the loss function can generate maximum loss for incorrect predictions but for SHA, you are essentially subtracting two random number vectors which will significantly reduce loss for incorrect predictions.

[Discussion](https://x.com/sytelus/status/1243131220994609154)
