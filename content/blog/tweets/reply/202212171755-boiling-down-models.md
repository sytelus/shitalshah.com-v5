---
title: 'Boiling Down Models: The Gains from Distillation'
draft: true
date: 2022-12-17T17:55:03+00:00
slug: '202212171755-boiling-down-models'
is_tweet: true
tweet_info:
  id: '1604052547031142402'
  type: 'reply'
  is_thread: False
---



{{< tweet user="_lewtun" id="1603770682021535744" >}}

What are the gains you see from distillation? For instance, if I train model from scratch vs from teacher using distillation. Also, what distillation technique(s) are you using? It would be great if you can provide distillation benchmark so one can reproduce.

[Discussion](https://x.com/sytelus/status/1604052547031142402)
