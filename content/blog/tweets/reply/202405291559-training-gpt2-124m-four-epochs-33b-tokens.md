---
title: 'Training GPT-2 124M: Four Epochs and 33 Billion Tokens Walk Into a Bar'
draft: true
date: 2024-05-29T15:59:40+00:00
slug: '202405291559-training-gpt2-124m-four-epochs-33b-tokens'
is_tweet: true
tweet_info:
  id: '1795741788839477377'
  type: 'reply'
  is_thread: False
---



{{< tweet user="karpathy" id="1795513568655487221" >}}

[@swyx](https://x.com/swyx) iirc, GPT2 124M can be trained for reproducible loss on 33B tokens  (~4 epochs on OpenWebText's 9B tokens).

[Discussion](https://x.com/sytelus/status/1795741788839477377)
