---
title: 'Money Burned, Code Spurned: LLaMA''s Training Flaw'
draft: true
date: 2023-04-18T10:55:48+00:00
slug: '202304181055-money-burned-code-spurned'
tags:
  - tweets
is_tweet: true
tweet_info:
  id: '1648173457149153282'
  type: 'reply'
  is_thread: False
---



{{< tweet user="abacaj" id="1648002419643523074" >}}

[@Francis_YAO_](https://x.com/Francis_YAO_) This is my understanding as well. I am not sure why LLaMA, Pythia etc keep ignoring this while burning $$$ on large scale training. It would be unlikely to truly match GPT-3.5 capabilities unless you include at least 100B tokens for code. You also need to start from code model asâ€¦ [continue reading](https://x.com/sytelus/status/1648173457149153282)
